Incident_Title,Incident_Link,Incident_color,Incident_Impact,Updates,Service
"Partial outage in some ada, davinci models",https://status.openai.com/incidents/ydpd7d9v63r7,#e86c09,impact-major,"[{""Update_Title"": ""Resolved"", ""Update_Body"": ""DALL\u00b7E and all APIs are operating normally."", ""Update_Timestamp"": ""Posted 2 years ago. Aug 30, 2022 - 11:57 PDT""}, {""Update_Title"": ""Monitoring"", ""Update_Body"": ""DALL\u00b7E is recovering and we are monitoring present traffic."", ""Update_Timestamp"": ""Posted 2 years ago. Aug 30, 2022 - 11:22 PDT""}, {""Update_Title"": ""Update"", ""Update_Body"": ""DALL\u00b7E remains unresponsive due to networking issues. We are actively diagnosing the situation. All other API traffic is operational."", ""Update_Timestamp"": ""Posted 2 years ago. Aug 30, 2022 - 11:10 PDT""}, {""Update_Title"": ""Update"", ""Update_Body"": ""We have moved text-ada-001 and classic davinci out of our affected cluster. These two models are now operational. Most DALL\u00b7E traffic is still heavily degraded."", ""Update_Timestamp"": ""Posted 2 years ago. Aug 30, 2022 - 10:33 PDT""}, {""Update_Title"": ""Update"", ""Update_Body"": ""All nodes in one of our clusters are experiencing DNS issues due to a systemd upgrade affecting the version of Ubuntu running on our nodes. This incident is also being tracked by our cloud provider here: https://app.azure.com/h/2TWN-VT0/c24ff9\n\nThis cluster handles most traffic for DALL\u00b7E as well as classic davinci and text-ada-001. These models are currently unreachable while we investigate workarounds."", ""Update_Timestamp"": ""Posted 2 years ago. Aug 30, 2022 - 10:26 PDT""}, {""Update_Title"": ""Identified"", ""Update_Body"": ""We have identified the issue and are working to resolve it.\n\n[edit] Timestamp updated to reflect when our monitoring reports the drop in traffic to our cluster."", ""Update_Timestamp"": ""Posted 2 years ago. Aug 30, 2022 - 09:16 PDT""}]",This incident affected: API.
Outage in text-davinci-002,https://status.openai.com/incidents/56m9dhrdmzq6,#ef4146,impact-critical,"[{""Update_Title"": ""Resolved"", ""Update_Body"": ""We have been stable since the previous message. Marking the incident as resolved."", ""Update_Timestamp"": ""Posted 2 years ago. Aug 25, 2022 - 17:21 PDT""}, {""Update_Title"": ""Monitoring"", ""Update_Body"": ""Performance appears to have mostly recovered at this point. We are continuing to monitor the situation."", ""Update_Timestamp"": ""Posted 2 years ago. Aug 25, 2022 - 15:59 PDT""}, {""Update_Title"": ""Update"", ""Update_Body"": ""Our deployment is progressing & models are continuing to recover. We are no longer seeing errors as frequently and latencies are dropping across the board. There is still degraded performance until the deployment completes."", ""Update_Timestamp"": ""Posted 2 years ago. Aug 25, 2022 - 15:51 PDT""}, {""Update_Title"": ""Identified"", ""Update_Body"": ""We are rolling out the fix and our models are in the process of recovering."", ""Update_Timestamp"": ""Posted 2 years ago. Aug 25, 2022 - 15:22 PDT""}, {""Update_Title"": ""Update"", ""Update_Body"": ""The fix we identified seems promising. Latency has been restored to the model we tested it on (text-babbage-001). We are now rolling it out more broadly."", ""Update_Timestamp"": ""Posted 2 years ago. Aug 25, 2022 - 15:02 PDT""}, {""Update_Title"": ""Update"", ""Update_Body"": ""We are continuing to investigate this issue. We also have observed that other models are experiencing increased latencies as well, though not to the point of failures.\n\nWe have a candidate fix that we are trying out now."", ""Update_Timestamp"": ""Posted 2 years ago. Aug 25, 2022 - 14:55 PDT""}, {""Update_Title"": ""Investigating"", ""Update_Body"": ""We began experienced intermittent failures in text-davinci-002 due to load beginning at 1:25 pm.\n\nWe are actively rearranging our capacity to allow these engines to recover."", ""Update_Timestamp"": ""Posted 2 years ago. Aug 25, 2022 - 13:25 PDT""}]",This incident affected: API.
Partial Outage on Account Management,https://status.openai.com/incidents/nkbpzgdp4sh5,#f4ac36,impact-minor,"[{""Update_Title"": ""Resolved"", ""Update_Body"": ""This issue has been resolved."", ""Update_Timestamp"": ""Posted 2 years ago. Aug 24, 2022 - 11:00 PDT""}, {""Update_Title"": ""Update"", ""Update_Body"": ""New accounts cannot be created and usage/billing information may be broken or stale."", ""Update_Timestamp"": ""Posted 2 years ago. Aug 24, 2022 - 10:28 PDT""}, {""Update_Title"": ""Update"", ""Update_Body"": ""We are continuing to work on a fix for this issue."", ""Update_Timestamp"": ""Posted 2 years ago. Aug 24, 2022 - 10:23 PDT""}, {""Update_Title"": ""Identified"", ""Update_Body"": ""New accounts cannot be created at the moment. We are currently investigating"", ""Update_Timestamp"": ""Posted 2 years ago. Aug 24, 2022 - 10:21 PDT""}]",This incident affected: API and Playground.
Outage on All Endpoints,https://status.openai.com/incidents/ljkf68byn8bm,#ef4146,impact-critical,"[{""Update_Title"": ""Postmortem"", ""Update_Body"": ""At 9:05 AM PDT on Friday, August 19, we experienced a full outage on our publicly exposed load balancer, knocking out traffic to all models and the Playground. All customers making API requests at this time were affected by this outage. The network outage lasted approximately one hour. Upon network traffic returning, some customers continued to see elevated errors when making requests to some models and to our Moderation API, and when making changes to their billing settings. Within two hours these cascading issues were fully resolved.\nEngineers quickly identified that the problem was related to our public load balancer. All visible configuration and monitoring indicated that the load balancer was operating correctly. We escalated to our cloud provider to help with the investigation, who later determined that an unrelated change to our network configuration broke our public load balancer in a way that was not visible to us.\nApproximately half of traffic was restored an hour after the incident began. But unfortunately after such a long period of outage, internal automation then further hindered our ability to serve the full load. Over the subsequent two hours, engineers manually worked to get all systems back online.\nThis multi-hour outage went on for far too long. To enable faster recovery times in the future, we're implementing changes to further increase observability and increase robustness of our change control processes.\n\u200c\nIn the course of investigation, engineers have identified the underlying issue in how network configurations get propagated that caused the load balancer to unexpectedly break, and have been able to reproduce the issue in a test environment. A fix is expected soon, and in the meantime we are able to reliably mitigate the bug."", ""Update_Timestamp"": ""Posted 2 years ago. Aug 25, 2022 - 14:07 PDT""}, {""Update_Title"": ""Resolved"", ""Update_Body"": ""This issue has been resolved. We apologize again for the inconvenience. We have begun a postmortem into this outage and will share it out as soon as we're able."", ""Update_Timestamp"": ""Posted 2 years ago. Aug 19, 2022 - 12:15 PDT""}, {""Update_Title"": ""Monitoring"", ""Update_Body"": ""A fix has been implemented and we are monitoring the results."", ""Update_Timestamp"": ""Posted 2 years ago. Aug 19, 2022 - 10:56 PDT""}, {""Update_Title"": ""Investigating"", ""Update_Body"": ""Network traffic is flowing again but many of our models are failing. We are investigating."", ""Update_Timestamp"": ""Posted 2 years ago. Aug 19, 2022 - 10:20 PDT""}, {""Update_Title"": ""Monitoring"", ""Update_Body"": ""We are seeing external traffic reach our servers again. We are monitoring."", ""Update_Timestamp"": ""Posted 2 years ago. Aug 19, 2022 - 10:08 PDT""}, {""Update_Title"": ""Update"", ""Update_Body"": ""We are experiencing a critical issue in our networking stack preventing external connections to our system. We are continuing to investigate. We appreciate your patience and apologize for the inconvenience."", ""Update_Timestamp"": ""Posted 2 years ago. Aug 19, 2022 - 09:52 PDT""}, {""Update_Title"": ""Investigating"", ""Update_Body"": ""We are investigating a major outage affecting all models and the OpenAI Playground"", ""Update_Timestamp"": ""Posted 2 years ago. Aug 19, 2022 - 09:20 PDT""}]",This incident affected: API and Playground.
Elevated errors on some davinci models,https://status.openai.com/incidents/xplpszlpsh5j,#e86c09,impact-major,"[{""Update_Title"": ""Resolved"", ""Update_Body"": ""This incident has been resolved."", ""Update_Timestamp"": ""Posted 2 years ago. Aug 18, 2022 - 18:16 PDT""}, {""Update_Title"": ""Identified"", ""Update_Body"": ""The issue has been identified and a fix is being implemented."", ""Update_Timestamp"": ""Posted 2 years ago. Aug 18, 2022 - 16:55 PDT""}, {""Update_Title"": ""Investigating"", ""Update_Body"": ""Some customers are seeing elevated errors on certain davinci models. We are investigating."", ""Update_Timestamp"": ""Posted 2 years ago. Aug 18, 2022 - 16:48 PDT""}]",This incident affected: API.
Elevated errors on fine-tuned Babbage models,https://status.openai.com/incidents/sb9nhynw9jjf,#e86c09,impact-major,"[{""Update_Title"": ""Resolved"", ""Update_Body"": ""fine-tuned Babbage models are not showing any errors in the last 20 minutes and the systems look healthy. We're resolving this issue."", ""Update_Timestamp"": ""Posted 2 years ago. Aug 17, 2022 - 09:56 PDT""}, {""Update_Title"": ""Monitoring"", ""Update_Body"": ""A fix has been implemented and we are monitoring the results."", ""Update_Timestamp"": ""Posted 2 years ago. Aug 17, 2022 - 09:47 PDT""}, {""Update_Title"": ""Identified"", ""Update_Body"": ""We've identified the issue and are working on a resolution"", ""Update_Timestamp"": ""Posted 2 years ago. Aug 17, 2022 - 09:31 PDT""}, {""Update_Title"": ""Investigating"", ""Update_Body"": ""We are seeing elevated errors for some customers using fine-tuned Babbage models. We are looking into the issue."", ""Update_Timestamp"": ""Posted 2 years ago. Aug 17, 2022 - 09:23 PDT""}]",This incident affected: API.
partial network outage,https://status.openai.com/incidents/1yqdnt73snnc,#e86c09,impact-major,"[{""Update_Title"": ""Resolved"", ""Update_Body"": ""While performing a major migration in our infrastructure a handful of services were accidentally cutoff because of a failed upgrade.\nMost of the affected services recovered within 10min, text-davinci-002 was affected for 45min.\nDuring this outage, requests to affected services would result in 404 (not found)"", ""Update_Timestamp"": ""Posted 2 years ago. Aug 12, 2022 - 13:30 PDT""}]",
API Outage,https://status.openai.com/incidents/0sfg212mvr14,#e86c09,impact-major,"[{""Update_Title"": ""Resolved"", ""Update_Body"": ""Our API suffered an outage for 11 minutes (4:06-4:17 PM) due to an internal configuration issue. The issue was caught and a fix applied. We apologize for lost requests in that time frame. All requests during this time would have timed out."", ""Update_Timestamp"": ""Posted 2 years ago. Aug 08, 2022 - 16:18 PDT""}]",
Error responses for curie fine-tuned models,https://status.openai.com/incidents/0vbqdb8gmyvy,#f4ac36,impact-minor,"[{""Update_Title"": ""Postmortem"", ""Update_Body"": ""On Jul 22, 2022 starting 11:10pm, some fine-tuned curie models suffered an outage for around 50 minutes. The outage affected infrastructure for more recently fine-tuned curie models (older models are running on separate infrastructure and were unaffected). Overall this represented 80% of requests to curie fine-tuned models.\nThe cause was a failure of the pub/sub system we use to queue requests. Even though the specific pods responsible for pub/sub were brought back up, there were cascading failures that prevented traffic from recovering. \nThe mitigation involved standing up a new infrastructure to process these requests. This was brought up and put into production traffic at around midnight, at which point failures stopped. Because new systems have some startup costs and cold caches, there was still higher latency observed for around 10 minutes. \nRemediations:\nSince this incident, we have begun sharding traffic for fine-tuned models across different regions to add redundancy in case of an outage.\nWe are in the process of re-architecting our model runners to be more resilient to pub-sub failures."", ""Update_Timestamp"": ""Posted 2 years ago. Jul 26, 2022 - 09:54 PDT""}, {""Update_Title"": ""Resolved"", ""Update_Body"": ""This incident has been resolved."", ""Update_Timestamp"": ""Posted 2 years ago. Jul 23, 2022 - 00:15 PDT""}, {""Update_Title"": ""Update"", ""Update_Body"": ""Latency should be back to normal. Service is healthy."", ""Update_Timestamp"": ""Posted 2 years ago. Jul 23, 2022 - 00:08 PDT""}, {""Update_Title"": ""Update"", ""Update_Body"": ""We have identified the source of the issue and have applied mitigations to restore service. However, latency is still affected during warmup."", ""Update_Timestamp"": ""Posted 2 years ago. Jul 22, 2022 - 23:58 PDT""}, {""Update_Title"": ""Update"", ""Update_Body"": ""We are continuing to investigate this issue."", ""Update_Timestamp"": ""Posted 2 years ago. Jul 22, 2022 - 23:42 PDT""}, {""Update_Title"": ""Investigating"", ""Update_Body"": ""We have been alerted to an issue with curie fine-tuned models. We are investigating."", ""Update_Timestamp"": ""Posted 2 years ago. Jul 22, 2022 - 23:42 PDT""}]",This incident affected: API.
Authentication outage,https://status.openai.com/incidents/x7ypskmhwdmj,#050505,impact-none,"[{""Update_Title"": ""Resolved"", ""Update_Body"": ""Our API suffered a momentary outage of our authentication service for 4 minutes (10:22-10:26 AM) due to an internal configuration issue. The issue was caught and a fix applied. We apologize for lost requests in that time frame. All engines were operational, however, some users temporarily were denied access."", ""Update_Timestamp"": ""Posted 2 years ago. Jul 18, 2022 - 10:39 PDT""}]",This incident affected: API.
Outage on some Ada and Babbage models,https://status.openai.com/incidents/qz0h6b7jw268,#e86c09,impact-major,"[{""Update_Title"": ""Postmortem"", ""Update_Body"": ""Summary:\nOn July 6, 2022 22:17 PDT (July 7 05:17 UTC), the following models had partial or complete outages:\ntext-similarity-ada-001\ntext-search-babbage-doc-001\ncontent-filter-alpha\ntext-search-babbage-query-001\ntext-search-ada-doc-001\nModels were unavailable for one and a half hours. This was caused by an unanticipated maintenance event performed by Azure which took offline VMs that these models were served. Models were reconfigured to depend on VMs that were not taken offline by the maintenance event.\nRoot Cause:\nA misconfiguration in our Azure activity log alerts caused this scheduled maintenance to occur without prior notice on our end.\nFuture Mitigations:\nEvaluate the activity log notification configurations for our critical resources to ensure notifications are received in a timely manner."", ""Update_Timestamp"": ""Posted 2 years ago. Jul 11, 2022 - 18:03 PDT""}, {""Update_Title"": ""Resolved"", ""Update_Body"": ""All models have recovered and are operational.  Thank you for your patience."", ""Update_Timestamp"": ""Posted 2 years ago. Jul 07, 2022 - 01:12 PDT""}, {""Update_Title"": ""Monitoring"", ""Update_Body"": ""We\u2019ve moved affected models to other regions, and are seeing broad recovery. We are continuing to monitor while we investigate root cause."", ""Update_Timestamp"": ""Posted 2 years ago. Jul 06, 2022 - 23:56 PDT""}, {""Update_Title"": ""Update"", ""Update_Body"": ""We have lost the ability to communicate with virtual machines in one of the regions in which we operate. We are mitigating by moving capacity to other regions. We are starting to see recovery on some affected models."", ""Update_Timestamp"": ""Posted 2 years ago. Jul 06, 2022 - 23:44 PDT""}, {""Update_Title"": ""Identified"", ""Update_Body"": ""We have identified an issue affecting some Ada and Babbage models and are working on a remediation."", ""Update_Timestamp"": ""Posted 2 years ago. Jul 06, 2022 - 23:07 PDT""}]",This incident affected: API.
Cluster Outage,https://status.openai.com/incidents/7h9pjy5d1lz5,#e86c09,impact-major,"[{""Update_Title"": ""Postmortem"", ""Update_Body"": ""Summary:\nOn June 23, 2022 18:56 PDT (June 24 01:56 UTC), the following models went offline: \nSome Fine-tuned models: curie and davinci\nSome Codex beta models: code-davinci-001, code-cushman-001\nSome legacy GPT models: curie, babbage, and ada\nAll Embeddings models\nCodex, Embeddings, and Legacy GPT models were completely unavailable for about an hour. Fine-tuned Curie was failing for most requests for 5 hours. Fine-tuned Davinci was failing for most requests for 12 hours.\nThe main cause was that one of our three production clusters went completely offline. Once that cluster went down, we needed to re-allocate models to our remaining two clusters. The downed cluster took 10 hours to fully restore due to several unexpected events detailed below. While most models recovered before the cluster was restored, Fine-tuned Davinci was not able to fully recover until the cluster was restored.\nWhat happened to fine-tuned models:\nFine-tuned Curie was timing out for most requests until 00:10 PDT (5 hours). Fine-tuned Davinci was timing out for most requests until 06:40 PDT (12 hours). There were several reasons why fine-tuned models took so long to restore.\nWhile we normally keep healthy infrastructure headroom, these margins were exhausted with the loss of an entire cluster. For us GPUs are the most constrained resource. Due to fundamental supply chain constraints we are unable to provision more GPUs with hours of notice. While we were able to maximally use our remaining fleet, we would not be able to fully restore service without our downed cluster coming back online.\nEach fine-tuned model is very large in size. We have many fine-tuned models across all of our customers. Normally, we only load a few at a time from Azure Blob Storage. When we restore all fine-tuned services from scratch, we're massively bottlenecked by Azure Blob Storage bandwidth. This was made worse by cross-cluster data transfer. We normally have many layers of caching to alleviate this; however, the nature of this move invalidated all of those.\nWhat happened to cluster restoration:\nThe cluster went down due to a combination of human error, unforeseen holes in our safeguards, and subtly missing information about the impacts of destructive operations. It took us until about 05:30 PDT (10\u00bd hours) to restore this cluster. Our clusters are managed with Terraform and Kubernetes via Azure Kubernetes Service (AKS). Normally cluster creation is fairly quick. There were several unexpected issues that led to this taking a long time to restore.\nDuring cluster setup we lost connection to AKS. After working with Azure support, we ended up rebuilding our VPN link to mitigate. We later unexpectedly lost connection again to Azure Private Link. We worked with Azure support again to build workarounds. We do not yet have a root cause of these issues and are actively working with Azure on cause and remediations.\nA Kubernetes upgrade would alleviate some of these network problems. After performing the upgrade, we discovered our Ingress was incompatible with the new Kubernetes version. Fixing these ingress problems was not feasible given the time constraints, and we discovered that AKS does not allow Kubernetes downgrades.\nWe finally tore down and rebuilt the cluster again from scratch. This was successful and we began moving traffic back into this cluster between 05:30 and 06:30 PDT.\nRemediations:\nWe are reprioritizing other product and engineering efforts to immediately dedicate significant resources to the following:\nWe are actively underway with several remediations to prevent this type of accident from happening again, ensure recovery is faster, and overall improve the performance of our infrastructure.\nAdding checks and locks to prevent this and similar prod-critical pieces of infrastructure from being inadvertently modified. While we had some of these protections already in place, they were not comprehensive enough to cover the cluster failure mode that happened here.\nAdding better visibility to operations being performed and ensuring parts of our system are more lexicographically distinct.\nContinuing to work with Azure on AKS network connectivity issues to ensure this does not disrupt new cluster operations in the future.\nUpdating our playbooks with these new cluster failure modes and modifying new cluster and infrastructure setup steps.\nPerforming a systematic upgrade of Kubernetes and provisioning time to ensure we don't fall behind in the future.\nSharding our fine-tuned models across multiple clusters, and are working towards more automatic cross cluster replication strategies.\nDistributing and replicating Azure Blob Storage to ensure fine-tuned models can load faster and more reliably.\nFinish rolling out a node-to-node caching solution that can alleviate Azure Blob Storage bottlenecks.\nModifying the way we store model weights to be more efficient and faster to load.\nWe expect most of these remediations to be implemented within days and have reprioritized other efforts to immediately work on these improvements."", ""Update_Timestamp"": ""Posted 2 years ago. Jun 27, 2022 - 20:20 PDT""}, {""Update_Title"": ""Resolved"", ""Update_Body"": ""All models are operational.  Thank you for your patience."", ""Update_Timestamp"": ""Posted 2 years ago. Jun 24, 2022 - 08:25 PDT""}, {""Update_Title"": ""Update"", ""Update_Body"": ""Babbage is now stable. We're investigating 1-2 remaining issues with our fine tuned curie models and a few lesser used engines."", ""Update_Timestamp"": ""Posted 2 years ago. Jun 24, 2022 - 08:11 PDT""}, {""Update_Title"": ""Update"", ""Update_Body"": ""At this time davinci fine-tuned models should be back to normal. We're investigating an issue with our babbage engine."", ""Update_Timestamp"": ""Posted 2 years ago. Jun 24, 2022 - 07:30 PDT""}, {""Update_Title"": ""Update"", ""Update_Body"": ""We have brought back our original cluster and are bringing back traffic. As of this post, davinci fine-tuned models should be normalizing in latency and error rates."", ""Update_Timestamp"": ""Posted 2 years ago. Jun 24, 2022 - 06:46 PDT""}, {""Update_Title"": ""Update"", ""Update_Body"": ""Davinci fine-tuned models are coming back up but are seeing increased latency. We are continuing to work to resolve this outage."", ""Update_Timestamp"": ""Posted 2 years ago. Jun 24, 2022 - 05:24 PDT""}, {""Update_Title"": ""Update"", ""Update_Body"": ""We do not have a resolution on this incident but we are working with our upstream partners for support. Users of davinci fine-tuned models are still advised to use text-davinci-002 for the time being."", ""Update_Timestamp"": ""Posted 2 years ago. Jun 24, 2022 - 04:14 PDT""}, {""Update_Title"": ""Investigating"", ""Update_Body"": ""Fine-tuned Davinci model inference is still degraded. We are exploring alternate theories as to what is causing very high latency on these models. Given the set of root causes that have already been ruled out, this unfortunately is indicating that a much more extensive investigation will be needed to fully remediate fine-tuned Davinci model performance.\n\nWe suggest using the text-davinci-002 model as a temporary backup while we work to restore fine-tuned Davinci. The text-davinci-002 model is both fully operational and can approach the capability of fine-tuned Davinci models for many applications. \n\nAll other public production models are operating nominally and we have restored the original cluster that had an outage."", ""Update_Timestamp"": ""Posted 2 years ago. Jun 24, 2022 - 02:38 PDT""}, {""Update_Title"": ""Update"", ""Update_Body"": ""Fine-tuned curie model inference has returned to normal.\n\nFine-tuned davinci model inference is still in a degraded state."", ""Update_Timestamp"": ""Posted 2 years ago. Jun 24, 2022 - 01:10 PDT""}, {""Update_Title"": ""Monitoring"", ""Update_Body"": ""We are seeing error rates drop on curie fine-tuned models as well as davinci fine-tuned models. We're actively monitoring the situation."", ""Update_Timestamp"": ""Posted 2 years ago. Jun 24, 2022 - 00:26 PDT""}, {""Update_Title"": ""Identified"", ""Update_Body"": ""We are continuing to address health issues with fine-tuned curie and fine-tuned davinci models.\n\nIn addition to aforementioned model loading issues, we are experiencing limits in our capacity while we restore the cluster that went out.\n\nAll other models are operational."", ""Update_Timestamp"": ""Posted 2 years ago. Jun 23, 2022 - 23:45 PDT""}, {""Update_Title"": ""Monitoring"", ""Update_Body"": ""We believe we have found a stable arrangement of our infrastructure. All models are responding to requests; however, fine-tuned davinci and fine-tuned curie have an elevated rates of 429s and 499s.\n\nThe fine-tuned davinci and fine-tuned curie model errors are due to customer model weights taking a long time to load. Normally these weights are heavily cached; however, due to these cluster rearrangements, those caches need to be restored. The sudden influx of requests to restore those caches is causing slowdowns upstream from our storage accounts. We expect the error rates to steadily decline, but may take longer than normal due to these bottlenecks."", ""Update_Timestamp"": ""Posted 2 years ago. Jun 23, 2022 - 23:03 PDT""}, {""Update_Title"": ""Update"", ""Update_Body"": ""We are continuing to move infrastructure around in our operational clusters to ensure all models are performing optimally with the resources we have. We are much closer to a stable configuration, but are still re-allocating resources to better bring down error rates.\n\nSome Fine-tuned curie models are the most heavily affected right now as we continue to move resources around."", ""Update_Timestamp"": ""Posted 2 years ago. Jun 23, 2022 - 22:15 PDT""}, {""Update_Title"": ""Update"", ""Update_Body"": ""We have now moved all models from the broken cluster to new clusters; however, we are still suffering from some warmup and capacity issues.\n\nFine-tuned davinci and curie models are warming up. Their performance should improve over time and the rates of 429s and 499s should steadily decrease.\n\nWe're also experiencing capacity issues with Codex davinci and cushman engines. We are actively working to fix these. Until then, they will have degraded performance until these issues get resolved."", ""Update_Timestamp"": ""Posted 2 years ago. Jun 23, 2022 - 21:39 PDT""}, {""Update_Title"": ""Identified"", ""Update_Body"": ""One of our clusters has suffered a major communication outage within kubernetes. This has affected the models that are hosted in that cluster.\n\nThis includes the following models:\n- Inference for fine-tuned davinci and curie models\n- Codex: code-davinci-001, and code-cushman-001\n- Legacy curie, babbage, and ada\n- Embeddings models\n\nWe are actively working to migrate most of these models to a functioning cluster. Affected models should be coming online as this happens.\n\nDue to capacity constraints, we unfortunately expect to see some temporary performance and latency degradations in other models as we move infrastructure around."", ""Update_Timestamp"": ""Posted 2 years ago. Jun 23, 2022 - 21:14 PDT""}, {""Update_Title"": ""Update"", ""Update_Body"": ""We are currently in a state of degraded performance for most engines. We are still working to recover."", ""Update_Timestamp"": ""Posted 2 years ago. Jun 23, 2022 - 20:43 PDT""}, {""Update_Title"": ""Update"", ""Update_Body"": ""We know the source of the outage and are working to mitigate."", ""Update_Timestamp"": ""Posted 2 years ago. Jun 23, 2022 - 20:10 PDT""}, {""Update_Title"": ""Investigating"", ""Update_Body"": ""One of our clusters has had an outage affecting some engines. We are investigating."", ""Update_Timestamp"": ""Posted 2 years ago. Jun 23, 2022 - 19:02 PDT""}]",This incident affected: API.
Logged in websites unavailable,https://status.openai.com/incidents/jzk2gzhfc34g,#e86c09,impact-major,"[{""Update_Title"": ""Resolved"", ""Update_Body"": ""A fix has been implemented by our service provider. Playground & DALL-E are now accessible."", ""Update_Timestamp"": ""Posted 2 years ago. Jun 21, 2022 - 00:38 PDT""}, {""Update_Title"": ""Monitoring"", ""Update_Body"": ""Playground & DALL-E websites are currently unavailable due to an outage from our authentication service provider. We're currently monitoring the situation."", ""Update_Timestamp"": ""Posted 2 years ago. Jun 21, 2022 - 00:01 PDT""}, {""Update_Title"": ""Identified"", ""Update_Body"": ""Playground & DALL-E websites are currently unavailable due to an outage from our authentication service provider. We're currently monitoring the situation."", ""Update_Timestamp"": ""Posted 2 years ago. Jun 21, 2022 - 00:01 PDT""}, {""Update_Title"": ""Investigating"", ""Update_Body"": ""Investigating reports of Playground not being accessible."", ""Update_Timestamp"": ""Posted 2 years ago. Jun 21, 2022 - 00:00 PDT""}]",This incident affected: Playground.
Base babbage model currently down,https://status.openai.com/incidents/2bxby42r6wv4,#e86c09,impact-major,"[{""Update_Title"": ""Resolved"", ""Update_Body"": ""Serving of babbage models is now healthy. The cause of the issue appears to have been some sort of sporadic failure. We will continue to investigate root cause and future mitigations."", ""Update_Timestamp"": ""Posted 2 years ago. Jun 20, 2022 - 10:06 PDT""}, {""Update_Title"": ""Monitoring"", ""Update_Body"": ""The babbage models appear to have recovered, we are monitoring and continuing to investigate root cause."", ""Update_Timestamp"": ""Posted 2 years ago. Jun 20, 2022 - 09:52 PDT""}, {""Update_Title"": ""Investigating"", ""Update_Body"": ""Base Babbage models are currently down. This appears to currently apply to base babbage usage, not fine tuned babbage models."", ""Update_Timestamp"": ""Posted 2 years ago. Jun 20, 2022 - 09:34 PDT""}]",This incident affected: API and Playground.
Degradation in serving fine tuned models.,https://status.openai.com/incidents/4cmnntq2t89y,#e86c09,impact-major,"[{""Update_Title"": ""Resolved"", ""Update_Body"": ""Service should be back to normal after all rollbacks."", ""Update_Timestamp"": ""Posted 2 years ago. Jun 03, 2022 - 15:21 PDT""}, {""Update_Title"": ""Update"", ""Update_Body"": ""After rollbacks only babbage fine tuned models are still degraded. Still rolling back babbage."", ""Update_Timestamp"": ""Posted 2 years ago. Jun 03, 2022 - 15:19 PDT""}, {""Update_Title"": ""Update"", ""Update_Body"": ""We are continuing to investigate this issue."", ""Update_Timestamp"": ""Posted 2 years ago. Jun 03, 2022 - 14:20 PDT""}, {""Update_Title"": ""Investigating"", ""Update_Body"": ""A recent deploy has degraded serving of fine tuned models. We are currently rolling back and investigating the root cause of this issue."", ""Update_Timestamp"": ""Posted 2 years ago. Jun 03, 2022 - 14:15 PDT""}]",This incident affected: API and Playground.
